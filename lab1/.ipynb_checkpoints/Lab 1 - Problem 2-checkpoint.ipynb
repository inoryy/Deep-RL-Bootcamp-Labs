{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro_1",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Lab 1: Markov Decision Processes - Problem 2\n",
    "\n",
    "\n",
    "## Lab Instructions\n",
    "All your answers should be written in this notebook.  You shouldn't need to write or modify any other files.\n",
    "\n",
    "**You should execute every block of code to not miss any dependency.**\n",
    "\n",
    "\n",
    "*This project was developed by Peter Chen, Rocky Duan, Pieter Abbeel for the Berkeley Deep RL Bootcamp, August 2017. Bootcamp website with slides and lecture videos: https://sites.google.com/view/deep-rl-bootcamp/. It is adapted from Berkeley Deep RL Class [HW2](https://github.com/berkeleydeeprlcourse/homework/blob/c1027d83cd542e67ebed982d44666e0d22a00141/hw2/HW2.ipynb) [(license)](https://github.com/berkeleydeeprlcourse/homework/blob/master/LICENSE)*\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic setup\n",
    "from misc import FrozenLakeEnv, make_grader\n",
    "env = FrozenLakeEnv()\n",
    "import numpy as np, numpy.random as nr, gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3)\n",
    "class MDP(object):\n",
    "    def __init__(self, P, nS, nA, desc=None):\n",
    "        self.P = P # state transition and reward probabilities, explained below\n",
    "        self.nS = nS # number of states\n",
    "        self.nA = nA # number of actions\n",
    "        self.desc = desc # 2D array specifying what each grid cell means (used for plotting)\n",
    "mdp = MDP( {s : {a : [tup[:3] for tup in tups] for (a, tups) in a2d.items()} for (s, a2d) in env.P.items()}, env.nS, env.nA, env.desc)\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Policy Iteration\n",
    "\n",
    "The next task is to implement exact policy iteration (PI), which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $\\pi_0$\n",
    "\n",
    "For $n=0, 1, 2, \\dots$\n",
    "- Compute the state-value function $V^{\\pi_{n}}$\n",
    "- Using $V^{\\pi_{n}}$, compute the state-action-value function $Q^{\\pi_{n}}$\n",
    "- Compute new policy $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "---\n",
    "\n",
    "Below, you'll implement the first and second steps of the loop.\n",
    "\n",
    "### Problem 2a: state value function\n",
    "\n",
    "You'll write a function called `compute_vpi` that computes the state-value function $V^{\\pi}$ for an arbitrary policy $\\pi$.\n",
    "Recall that $V^{\\pi}$ satisfies the following linear equation:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "You can solve a linear system in your code. (Find an exact solution, e.g., with `np.linalg.solve`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_vpi(pi, mdp, gamma):\n",
    "    # use pi[state] to access the action that's prescribed by this policy\n",
    "    V = np.ones(mdp.nS) # REPLACE THIS LINE WITH YOUR CODE\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the value of an arbitrarily-chosen policy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected:  [  1.6e-02   2.4e-02   2.3e-01   2.4e-02   1.7e-02  -0.0e+00   3.0e-01\n",
      "   0.0e+00   2.0e-02   1.9e-01   3.9e-01   0.0e+00  -3.6e-17   2.0e-01\n",
      "   4.9e-01   0.0e+00]\n",
      "Actual:  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "expected_val = np.array([  1.381e-18,   1.844e-04,   1.941e-03,   1.272e-03,   2.108e-18,\n",
    "         0.000e+00,   8.319e-03,   1.727e-16,   3.944e-18,   2.768e-01,\n",
    "         8.562e-02,  -7.242e-16,   7.857e-18,   3.535e-01,   8.930e-01,\n",
    "         0.000e+00])\n",
    "\n",
    "actual_val = compute_vpi(np.arange(16) % mdp.nA, mdp, gamma=GAMMA)\n",
    "if np.all(np.isclose(actual_val, expected_val, atol=1e-4)):\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Expected: \", expected_val)\n",
    "    print(\"Actual: \", actual_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b: state-action value function\n",
    "\n",
    "Next, you'll write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "\n",
    "$$Q^{\\pi}(s, a) = \\sum_{s'} P(s,a,s')[ R(s,a,s') + \\gamma V^{\\pi}(s')]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "compute_qpi",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected:  [[  0.4   3.1   1.1   0.1]\n",
      " [  0.6   4.    2.1   0.9]\n",
      " [  1.5   4.9   3.    1.9]\n",
      " [  2.5   5.8   3.2   2.8]\n",
      " [  3.8   6.9   4.6   0.9]\n",
      " [  4.8   4.8   4.8   4.8]\n",
      " [  4.9   8.7   6.5   2.7]\n",
      " [  6.7   6.7   6.7   6.7]\n",
      " [  7.6  10.7   8.4   4.7]\n",
      " [  7.8  11.6   9.3   5.5]\n",
      " [  8.7  12.5  10.3   6.5]\n",
      " [ 10.4  10.4  10.4  10.4]\n",
      " [ 11.4  11.4  11.4  11.4]\n",
      " [ 11.2  12.3  12.7   9.3]\n",
      " [ 12.2  13.4  14.5  10.4]\n",
      " [ 14.2  14.2  14.2  14.2]]\n",
      "Actual:  [[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def compute_qpi(vpi, mdp, gamma):\n",
    "    Qpi = np.zeros([mdp.nS, mdp.nA]) # REPLACE THIS LINE WITH YOUR CODE\n",
    "    return Qpi\n",
    "\n",
    "expected_Qpi = np.array([[  0.38 ,   3.135,   1.14 ,   0.095],\n",
    "       [  0.57 ,   3.99 ,   2.09 ,   0.95 ],\n",
    "       [  1.52 ,   4.94 ,   3.04 ,   1.9  ],\n",
    "       [  2.47 ,   5.795,   3.23 ,   2.755],\n",
    "       [  3.8  ,   6.935,   4.56 ,   0.855],\n",
    "       [  4.75 ,   4.75 ,   4.75 ,   4.75 ],\n",
    "       [  4.94 ,   8.74 ,   6.46 ,   2.66 ],\n",
    "       [  6.65 ,   6.65 ,   6.65 ,   6.65 ],\n",
    "       [  7.6  ,  10.735,   8.36 ,   4.655],\n",
    "       [  7.79 ,  11.59 ,   9.31 ,   5.51 ],\n",
    "       [  8.74 ,  12.54 ,  10.26 ,   6.46 ],\n",
    "       [ 10.45 ,  10.45 ,  10.45 ,  10.45 ],\n",
    "       [ 11.4  ,  11.4  ,  11.4  ,  11.4  ],\n",
    "       [ 11.21 ,  12.35 ,  12.73 ,   9.31 ],\n",
    "       [ 12.16 ,  13.4  ,  14.48 ,  10.36 ],\n",
    "       [ 14.25 ,  14.25 ,  14.25 ,  14.25 ]])\n",
    "\n",
    "Qpi = compute_qpi(np.arange(mdp.nS), mdp, gamma=0.95)\n",
    "if np.all(np.isclose(expected_Qpi, Qpi, atol=1e-4)):\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Expected: \", expected_Qpi)\n",
    "    print(\"Actual: \", Qpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to run policy iteration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration | # chg actions | V[0]\n",
      "----------+---------------+---------\n",
      "\u001b[41m   0      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   0      |      1        | -0.00000\u001b[0m\n",
      "\u001b[41m   1      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   1      |      9        | 0.00000\u001b[0m\n",
      "\u001b[41m   2      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   2      |      2        | 0.39785\u001b[0m\n",
      "\u001b[41m   3      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   3      |      1        | 0.45546\u001b[0m\n",
      "\u001b[41m   4      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   4      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m   5      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   5      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m   6      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   6      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m   7      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   7      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m   8      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   8      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m   9      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m   9      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  10      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  10      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  11      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  11      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  12      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  12      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  13      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  13      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  14      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  14      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  15      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  15      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  16      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  16      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  17      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  17      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  18      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  18      |      0        | 0.53118\u001b[0m\n",
      "\u001b[41m  19      |      0        | 1.00000\u001b[0m *** Expected: \u001b[42m  19      |      0        | 0.53118\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD8FJREFUeJzt23+MZWV9x/H3R5bVRFF+7IbS3a0rLW1cG6vbcUUrstEG\nd0nDVtJYqAk/bLIxQlP/IA2GRAzGGH81DdZA1naDqAWsVbttMUBRwz+uZVBYQAQGomXXlR2LYAl/\nWPDbP+5Zch1n5t6duTN3luf9Sm7mnOd5zj3feebczz33nDupKiRJbXjRuAuQJC0fQ1+SGmLoS1JD\nDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUkFXjLmCmNWvW1MaNG8ddhiQdVe66666fVtXaQeNW\nXOhv3LiRycnJcZchSUeVJD8aZpyXdySpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kN\nMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBD\nX5IaYuhLUkMMfUlqiKEvSQ0ZGPpJdic5lOS+OfqT5OokU0n2Jdk8o//lSfYn+ftRFS1JWphhzvSv\nA7bN078dOK177ASumdH/YeCOhRQnSRqtgaFfVXcAT8wzZAdwffXsBY5PcgpAkj8ETgZuHUWxkqTF\nGcU1/XXAY33r+4F1SV4EfAq4bAT7kCSNwFLeyH0fcHNV7R80MMnOJJNJJqenp5ewJElq26oRPMcB\nYEPf+vqu7U3AGUneB7wMWJ3k6aq6fOYTVNUuYBfAxMREjaAmSdIsRhH6e4BLk9wIvBF4qqoOAu8+\nPCDJRcDEbIEvSVo+A0M/yQ3AVmBNkv3AlcCxAFV1LXAzcDYwBTwDXLxUxUqSFmdg6FfV+QP6C7hk\nwJjr6H31U5I0Rv5HriQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SG\nGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1Jaoih\nL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDRkY+kl2JzmU5L45+pPk6iRT\nSfYl2dy1vy7Jt5Pc37X/+aiLlyQdmWHO9K8Dts3Tvx04rXvsBK7p2p8BLqiq13Tb/12S4xdeqiRp\nsVYNGlBVdyTZOM+QHcD1VVXA3iTHJzmlqh7qe44fJzkErAWeXGTNkqQFGsU1/XXAY33r+7u25yXZ\nAqwGHhnB/iRJC7TkN3KTnAJ8Hri4qn45x5idSSaTTE5PTy91SZLUrFGE/gFgQ9/6+q6NJC8H/gO4\noqr2zvUEVbWrqiaqamLt2rUjKEmSNJtRhP4e4ILuWzynA09V1cEkq4Gv0rve/+UR7EeStEgDb+Qm\nuQHYCqxJsh+4EjgWoKquBW4Gzgam6H1j5+Ju03cBbwVOSnJR13ZRVd09wvolSUdgmG/vnD+gv4BL\nZmn/AvCFhZcmSRo1/yNXkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhL\nUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1\nxNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNWRg6CfZneRQkvvm6E+Sq5NMJdmX\nZHNf34VJHu4eF46ycEnSkRvmTP86YNs8/duB07rHTuAagCQnAlcCbwS2AFcmOWExxUqSFmdg6FfV\nHcAT8wzZAVxfPXuB45OcArwDuK2qnqiqnwG3Mf+bhyRpia0awXOsAx7rW9/ftc3VvmQ+/Rfv4blf\nPreUu5CkJXPMi47hr/5p95LuY0XcyE2yM8lkksnp6elxlyNJL1ijONM/AGzoW1/ftR0Ats5o/9Zs\nT1BVu4BdABMTE7XQQpb6HVKSjnajONPfA1zQfYvndOCpqjoI3AKcleSE7gbuWV2bJGlMBp7pJ7mB\n3hn7miT76X0j51iAqroWuBk4G5gCngEu7vqeSPJh4M7uqa6qqvluCEuSltjA0K+q8wf0F3DJHH27\nAa+5SNIKsSJu5EqSloehL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0\nJakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+S\nGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhoyVOgn2ZbkwSRTSS6fpf+VSW5P\nsi/Jt5Ks7+v7eJL7kzyQ5OokGeUvIEka3sDQT3IM8BlgO7AJOD/JphnDPglcX1WvBa4CPtpt+2bg\nj4DXAr8PvAE4c2TVS5KOyDBn+luAqap6tKp+AdwI7JgxZhPwjW75m339BbwEWA28GDgWeHyxRUuS\nFmaY0F8HPNa3vr9r63cPcG63/E7guCQnVdW36b0JHOwet1TVA4srWZK0UKO6kXsZcGaS79G7fHMA\neC7J7wCvBtbTe6N4W5IzZm6cZGeSySST09PTIypJkjTTMKF/ANjQt76+a3teVf24qs6tqtcDV3Rt\nT9I7699bVU9X1dPA14E3zdxBVe2qqomqmli7du0CfxVJ0iDDhP6dwGlJXpVkNXAesKd/QJI1SQ4/\n1weA3d3yf9P7BLAqybH0PgV4eUeSxmRg6FfVs8ClwC30AvtLVXV/kquSnNMN2wo8mOQh4GTgI137\nl4FHgHvpXfe/p6r+bbS/giRpWKmqcdfwKyYmJmpycnLcZUjSUSXJXVU1MWic/5ErSQ0x9CWpIYa+\nJDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtS\nQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE\n0Jekhhj6ktQQQ1+SGjJU6CfZluTBJFNJLp+l/5VJbk+yL8m3kqzv6/utJLcmeSDJ95NsHF35kqQj\nMTD0kxwDfAbYDmwCzk+yacawTwLXV9VrgauAj/b1XQ98oqpeDWwBDo2icEnSkRvmTH8LMFVVj1bV\nL4AbgR0zxmwCvtEtf/Nwf/fmsKqqbgOoqqer6pmRVC5JOmLDhP464LG+9f1dW797gHO75XcCxyU5\nCfhd4MkkX0nyvSSf6D45SJLGYFQ3ci8DzkzyPeBM4ADwHLAKOKPrfwNwKnDRzI2T7EwymWRyenp6\nRCVJkmYaJvQPABv61td3bc+rqh9X1blV9Xrgiq7tSXqfCu7uLg09C3wN2DxzB1W1q6omqmpi7dq1\nC/xVJEmDDBP6dwKnJXlVktXAecCe/gFJ1iQ5/FwfAHb3bXt8ksNJ/jbg+4svW5K0EANDvztDvxS4\nBXgA+FJV3Z/kqiTndMO2Ag8meQg4GfhIt+1z9C7t3J7kXiDAZ0f+W0iShpKqGncNv2JiYqImJyfH\nXYYkHVWS3FVVE4PG+R+5ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENf\nkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWp\nIYa+JDXE0Jekhhj6ktSQVNW4a/gVSaaBHy3iKdYAPx1ROUvB+hbH+hbH+hZnJdf3yqpaO2jQigv9\nxUoyWVUT465jLta3ONa3ONa3OCu9vmF4eUeSGmLoS1JDXoihv2vcBQxgfYtjfYtjfYuz0usb6AV3\nTV+SNLcX4pm+JGkOR2XoJ9mW5MEkU0kun6X/xUlu6vq/k2TjMta2Ick3k3w/yf1J/nqWMVuTPJXk\n7u7xweWqr6+GHya5t9v/5Cz9SXJ1N4f7kmxextp+r29u7k7y8yTvnzFmWecwye4kh5Lc19d2YpLb\nkjzc/Txhjm0v7MY8nOTCZazvE0l+0P39vprk+Dm2nfdYWML6PpTkQN/f8Ow5tp339b6E9d3UV9sP\nk9w9x7ZLPn8jVVVH1QM4BngEOBVYDdwDbJox5n3Atd3yecBNy1jfKcDmbvk44KFZ6tsK/PuY5/GH\nwJp5+s8Gvg4EOB34zhj/3j+h9x3ksc0h8FZgM3BfX9vHgcu75cuBj82y3YnAo93PE7rlE5apvrOA\nVd3yx2arb5hjYQnr+xBw2RB//3lf70tV34z+TwEfHNf8jfJxNJ7pbwGmqurRqvoFcCOwY8aYHcDn\nuuUvA29PkuUorqoOVtV3u+X/BR4A1i3HvkdsB3B99ewFjk9yyhjqeDvwSFUt5h/2Fq2q7gCemNHc\nf5x9DvjTWTZ9B3BbVT1RVT8DbgO2LUd9VXVrVT3bre4F1o96v8OaY/6GMczrfdHmq6/LjncBN4x6\nv+NwNIb+OuCxvvX9/HqoPj+mO+ifAk5alur6dJeVXg98Z5buNyW5J8nXk7xmWQvrKeDWJHcl2TlL\n/zDzvBzOY+4X27jn8OSqOtgt/wQ4eZYxK2Ue30Pvk9tsBh0LS+nS7vLT7jkuj62E+TsDeLyqHp6j\nf5zzd8SOxtA/KiR5GfAvwPur6uczur9L73LFHwCfBr623PUBb6mqzcB24JIkbx1DDfNKsho4B/jn\nWbpXwhw+r3qf81fkV+GSXAE8C3xxjiHjOhauAX4beB1wkN4llJXofOY/y1/xr6V+R2PoHwA29K2v\n79pmHZNkFfAK4H+WpbrePo+lF/hfrKqvzOyvqp9X1dPd8s3AsUnWLFd93X4PdD8PAV+l9zG63zDz\nvNS2A9+tqsdndqyEOQQeP3zJq/t5aJYxY53HJBcBfwK8u3tj+jVDHAtLoqoer6rnquqXwGfn2O+4\n528VcC5w01xjxjV/C3U0hv6dwGlJXtWdCZ4H7JkxZg9w+FsSfwZ8Y64DftS663//CDxQVX87x5jf\nOHyPIckWen+H5XxTemmS4w4v07vhd9+MYXuAC7pv8ZwOPNV3KWO5zHmGNe457PQfZxcC/zrLmFuA\ns5Kc0F2+OKtrW3JJtgF/A5xTVc/MMWaYY2Gp6uu/R/TOOfY7zOt9Kf0x8IOq2j9b5zjnb8HGfSd5\nIQ963yx5iN5d/Su6tqvoHdwAL6F3SWAK+C/g1GWs7S30PubvA+7uHmcD7wXe2425FLif3jcR9gJv\nXub5O7Xb9z1dHYfnsL/GAJ/p5vheYGKZa3wpvRB/RV/b2OaQ3pvPQeD/6F1X/kt694luBx4G/hM4\nsRs7AfxD37bv6Y7FKeDiZaxvit718MPH4eFvtP0mcPN8x8Iy1ff57tjaRy/IT5lZX7f+a6/35aiv\na7/u8DHXN3bZ52+UD/8jV5IacjRe3pEkLZChL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtS\nQ/4fHrgO+IGVKuMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109c0f2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def policy_iteration(mdp, gamma, nIt, grade_print=print):\n",
    "    Vs = []\n",
    "    pis = []\n",
    "    pi_prev = np.zeros(mdp.nS,dtype='int')\n",
    "    pis.append(pi_prev)\n",
    "    grade_print(\"Iteration | # chg actions | V[0]\")\n",
    "    grade_print(\"----------+---------------+---------\")\n",
    "    for it in range(nIt):        \n",
    "        # YOUR CODE HERE\n",
    "        # you need to compute qpi which is the state-action values for current pi\n",
    "        pi = qpi.argmax(axis=1)\n",
    "        grade_print(\"%4i      | %6i        | %6.5f\"%(it, (pi != pi_prev).sum(), vpi[0]))\n",
    "        Vs.append(vpi)\n",
    "        pis.append(pi)\n",
    "        pi_prev = pi\n",
    "    return Vs, pis\n",
    "\n",
    "expected_output = \"\"\"Iteration | # chg actions | V[0]\n",
    "----------+---------------+---------\n",
    "   0      |      1        | -0.00000\n",
    "   1      |      9        | 0.00000\n",
    "   2      |      2        | 0.39785\n",
    "   3      |      1        | 0.45546\n",
    "   4      |      0        | 0.53118\n",
    "   5      |      0        | 0.53118\n",
    "   6      |      0        | 0.53118\n",
    "   7      |      0        | 0.53118\n",
    "   8      |      0        | 0.53118\n",
    "   9      |      0        | 0.53118\n",
    "  10      |      0        | 0.53118\n",
    "  11      |      0        | 0.53118\n",
    "  12      |      0        | 0.53118\n",
    "  13      |      0        | 0.53118\n",
    "  14      |      0        | 0.53118\n",
    "  15      |      0        | 0.53118\n",
    "  16      |      0        | 0.53118\n",
    "  17      |      0        | 0.53118\n",
    "  18      |      0        | 0.53118\n",
    "  19      |      0        | 0.53118\"\"\"\n",
    "\n",
    "Vs_PI, pis_PI = policy_iteration(mdp, gamma=0.95, nIt=20, grade_print=make_grader(expected_output))\n",
    "plt.plot(Vs_PI);"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
